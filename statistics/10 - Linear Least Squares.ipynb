{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Least Squares\n",
    "Correlation coefficients measure the strength and sign of a relationship, but not the slope. One way to estimate the slope is by using a **linear least squares fit**. \n",
    "- The **linear fit** is a line modeling the relationships between variables\n",
    "- The **least squares** fit  is the one minimizing the mean squared error (MSE) between the line and the data\n",
    "\n",
    "Say we have some x- and y-values representing different variables. IF there is a linear relationship between them, then we expect each y-value to be related to an x-value through `intercept + slope * xs[i]`. We can then find the **residual**, or the vertical deviation from the line, using:\n",
    "\n",
    "```\n",
    "res = ys - (intercept + slope * xs)\n",
    "```\n",
    "\n",
    "The residuals might be due to random factors like measurement error, or non-random factors that are unknown. For example, if we are trying to predict someone's weight based on their height, unknown factors might include diet, exercise, and body type.\n",
    "\n",
    "If we get the parameters `intercept` and `slope` wrong, the residuals get bigger, so we want the parameters that minimize the residuals. The most common choice when minimizing residuals is minimizing the sum of squared residuals, `sum(res**2)`. Why?\n",
    "- Squaring will treat the positive and negative residuals the same\n",
    "- Squaring gives more weight to large residuals, but not so much that the largest residual always dominates\n",
    "- If the residuals are uncorrelated and normally distributed with mean 0 and constant, unknown variance, then the least squares fit is also the maximum likelihood estimator of `intercept` and `slope`.\n",
    "\n",
    "Let's see this in action. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.017451920308772035 6.830445129040682\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def Covariance(xSample, ySample):\n",
    "    xMean = xSample.mean()\n",
    "    yMean = ySample.mean()\n",
    "    return np.dot(xSample - xMean, ySample - yMean) / xSample.count()\n",
    "\n",
    "df = pd.read_pickle(\"nsfg_data.pkl\")\n",
    "live = df[df[\"outcome\"] == 1]\n",
    "noNan = live[(live[\"agepreg\"].isna() == False) & (live[\"totalwgt_lb\"].isna() == False)]\n",
    "ages = noNan[\"agepreg\"]\n",
    "birthWgt = noNan[\"totalwgt_lb\"]\n",
    "\n",
    "xVar = np.var(ages, ddof=1)\n",
    "xMean = np.mean(ages)\n",
    "yMean = np.mean(birthWgt)\n",
    "\n",
    "# Here, we calculate the slope and intercept for the linear fit for these two variables\n",
    "slope = Covariance(ages, birthWgt) / xVar # The \"n's\" used to calculate both of these cancel out\n",
    "intercept = yMean - slope * xMean\n",
    "\n",
    "print(slope, intercept)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
