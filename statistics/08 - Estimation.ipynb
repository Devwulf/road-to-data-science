{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimation\n",
    "If we are given a random sample from a distribution, and we are asked to find the *mean* of the distribution itself, one way we could do so is to use the *mean* of the sample as an estimate of the *mean* of the distribution. This process is called **estimation**, and the statistic that we used, which is the sample mean, is the **estimator**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15516666666666667"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "randomSample = pd.Series([-0.441, 1.774, -0.101, -1.138, 2.975, -2.138])\n",
    "randomSample.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, what if the random sample of a distribution had outliers? Would taking the mean of the sample to estimate the mean of the distribution be the best choice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-35.121833333333335"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomSample2 = pd.Series([-0.441, 1.774, -0.101, -1.138, 2.975, -213.8])\n",
    "randomSample2.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the sample mean has been skewed by an outlier, which also affects our estimation of the distribution mean.\n",
    "\n",
    "One option is to identify and discard outliers, then compute the sample mean of the values left. Here, we're keeping all the values within 4 units from 0, and discarding the rest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6138"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomSample3 = randomSample2.where(np.abs(randomSample2) < 4)\n",
    "randomSample3.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another option is to use the median as an estimator, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.271"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "randomSample2.median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Picking the best estimator depends on the circumstances, like whether there are outliers, and on what the goal is.\n",
    "\n",
    "If there are no outliers, the sample mean minimizes the **mean squared error**. In this case, we compute the root of the mean squared error to nullify the squaring that we do. It looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RootMeanSqrError(estimates, actual):\n",
    "    # Squared error between the estimates and the actual value\n",
    "    sqrErrors = [(estimate - actual)**2 for estimate in estimates]\n",
    "    sum = np.sum(sqrErrors)\n",
    "    avg = sum / len(estimates)\n",
    "    sqrt = np.sqrt(avg)\n",
    "    return sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's simulate getting a random sample of size 7, 1000 times. In the function below, *n* is the size of the random sample, and *m* is the amount of simulations we'll be doing. For this example, we're also getting the random sample from a normal (gaussian) distribution with `mu = 0` and `sigma = 1`, since we're pretending that our out-of-sample follows this normal distribution. Note that *mu* is the mean of the normal distribution, while *sigma* is the standard deviation.\n",
    "\n",
    "Between mean and median, let's see which one is the best estimator of the distribution mean, given that there are no outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE of means: 0.3748377444328869\n",
      "RMSE of medians: 0.45378614029744924\n"
     ]
    }
   ],
   "source": [
    "mu = 0\n",
    "sigma = 1\n",
    "n = 7\n",
    "m = 1000\n",
    "\n",
    "means = []\n",
    "medians = []\n",
    "for i in range(m):\n",
    "    values = np.random.normal(mu, sigma, n)\n",
    "    means.append(np.mean(values))\n",
    "    medians.append(np.median(values))\n",
    "    \n",
    "print(\"RMSE of means:\", RootMeanSqrError(means, mu))\n",
    "print(\"RMSE of medians:\", RootMeanSqrError(medians, mu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, the error from estimation of the distribution mean using the sample means, at about `0.38`, is lower than that of medians, at about `0.45`.\n",
    "\n",
    "For some problems, it is nice to be able to minimize the MSE, but it is not always the best strategy. It still depends on the type of problem to decide whether or not to use the MSE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Guess the Variance\n",
    "Given a random sample, the variance of the distribution could be estimated by first calculating the variance of each value from the mean, squaring these, then getting the average of these values, like so: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance: 2.9873351388888882\n",
      "Numpy Variance: 2.9873351388888882\n"
     ]
    }
   ],
   "source": [
    "def Variance(sample):\n",
    "    mean = np.mean(sample)\n",
    "    diffs = [(value - mean)**2 for value in sample]\n",
    "    sum = np.sum(diffs)\n",
    "    avg = sum / len(diffs)\n",
    "    return avg\n",
    "\n",
    "print(\"Variance:\", Variance(randomSample))\n",
    "print(\"Numpy Variance:\", np.var(randomSample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For large enough samples, this is a good enough estimator, but for small samples, the value given tends to be too low. This is therefore called a **biased estimator**. An estimator is **unbiased** if the mean error, after many iterations, is 0.\n",
    "\n",
    "There is another estimator of the variance that is unbiased. Here, we also compare it with numpy's unbiased variance calculation using the `ddof` parameter, which uses `n - ddof` when calculating the average:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unbiased Variance: 3.584802166666666\n",
      "Numpy Unbiased Variance: 3.584802166666666\n"
     ]
    }
   ],
   "source": [
    "def VarianceUnbiased(sample):\n",
    "    mean = np.mean(sample)\n",
    "    diffs = [(value - mean)**2 for value in sample]\n",
    "    sum = np.sum(diffs)\n",
    "    avg = sum / (len(diffs) - 1)\n",
    "    return avg\n",
    "\n",
    "print(\"Unbiased Variance:\", VarianceUnbiased(randomSample))\n",
    "print(\"Numpy Unbiased Variance:\", np.var(randomSample, ddof=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the difference? We have simply subtracted the length of the sample, usually *n*, by 1. \n",
    "\n",
    "Let's simulate a random sample of size 7, 1000 times again. This time, we'll compare the errors of both the biased and unbiased variance estimators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean error of biased: -0.16390481348808378\n",
      "Mean error of unbiased: -0.02455561573609773\n"
     ]
    }
   ],
   "source": [
    "def MeanError(estimates, actual):\n",
    "    errors = [estimate - actual for estimate in estimates]\n",
    "    return np.mean(errors)\n",
    "\n",
    "mu = 0\n",
    "sigma = 1\n",
    "n = 7\n",
    "m = 1000\n",
    "\n",
    "biased = []\n",
    "unbiased = []\n",
    "for i in range(1000):\n",
    "    values = np.random.normal(mu, sigma, n)\n",
    "    biased.append(Variance(values)) # or using np.var(values)\n",
    "    unbiased.append(VarianceUnbiased(values)) # or using np.var(values, ddof=1)\n",
    "    \n",
    "actualVariance = sigma**2 # Variance is also standard deviation squared\n",
    "\n",
    "print(\"Mean error of biased:\", MeanError(biased, actualVariance))\n",
    "print(\"Mean error of unbiased:\", MeanError(unbiased, actualVariance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, the unbiased estimation is much closer to 0 than the biased estimation. As the number of iterations, or *m*, increases, we expect the mean error to approach 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Properties like the MSE and bias are long-term expectations based on many iterations of the estimation game. But when you apply the estimator to real data, you only get one estimate. It would not be meaningful to say that the estimate is biased or unbiased, since being biased or unbiased is a property of the estimator, not the estimate. \n",
    "\n",
    "After choosing an estimator with appropriate properties and use it to generate an estimate, the next step is finding the uncertainty of the estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
